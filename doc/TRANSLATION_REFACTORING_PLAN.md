# Translation Module Refactoring Plan

**Date:** 2025-12-06
**Status:** üöß **70% COMPLETE - In Progress**
**Last Updated:** 2025-12-06
**Goal:** Refactor translation functionality into a modular package supporting multiple AI backends (OpenAI, Gemini)

---

## üìä Implementation Progress

| Phase | Status | Progress | Completion Date |
|-------|--------|----------|-----------------|
| **Phase 1:** Core Infrastructure | ‚úÖ Complete | 100% | 2025-12-06 |
| **Phase 2:** OpenAI Provider | ‚úÖ Complete | 100% | 2025-12-06 |
| **Phase 3:** Gemini Provider | ‚úÖ Complete | 100% | 2025-12-06 |
| **Phase 4:** Service Layer | üöß Partial | 75% | In Progress |
| **Phase 5:** Integration & Migration | üöß Partial | 30% | In Progress |
| **Phase 6:** Documentation & Cleanup | ‚úÖ Complete | 90% | 2025-12-06 |
| **Overall** | üöß **In Progress** | **70%** | ETA: 1-2 weeks |

### ‚úÖ What's Complete

- ‚úÖ Full provider abstraction layer (OpenAI + Gemini)
- ‚úÖ Unified data models and exception hierarchy
- ‚úÖ Dynamic provider registry
- ‚úÖ Configuration management system
- ‚úÖ Analysis service (fully functional)
- ‚úÖ Comprehensive documentation (20,000+ lines)
- ‚úÖ Backward compatibility wrapper
- ‚úÖ Updated Django settings

### ‚è≥ What's Remaining

- ‚è≥ Translation service port (next priority)
- ‚è≥ Update Celery tasks
- ‚è≥ Comprehensive unit tests
- ‚è≥ Integration testing
- ‚è≥ Performance benchmarks

**See [AI_SERVICES_IMPLEMENTATION_STATUS.md](AI_SERVICES_IMPLEMENTATION_STATUS.md) for detailed status.**

---

## Table of Contents

1. [Current Architecture Analysis](#current-architecture-analysis)
2. [Problems with Current Design](#problems-with-current-design)
3. [Proposed Architecture](#proposed-architecture)
4. [Implementation Plan](#implementation-plan)
5. [Migration Strategy](#migration-strategy)
6. [Testing Strategy](#testing-strategy)
7. [Configuration](#configuration)
8. [Timeline & Phases](#timeline--phases)

---

## Current Architecture Analysis

### Current File Structure

```
myapp/books/utils/
‚îú‚îÄ‚îÄ base_ai_service.py          # Base class for AI services
‚îú‚îÄ‚îÄ chapter_analysis.py         # Entity extraction & summarization
‚îú‚îÄ‚îÄ chapter_translation.py      # Chapter translation logic
‚îú‚îÄ‚îÄ job_concurrency.py          # Concurrency management
‚îî‚îÄ‚îÄ __init__.py                 # Package exports
```

### Current Components

#### 1. **BaseAIService** (`base_ai_service.py`)

**Purpose:** Shared initialization and configuration for AI-powered services

**Key Features:**
- OpenAI client initialization
- Configuration management (model, max_tokens, temperature)
- Settings validation
- Subclass customization via setting names

**Dependencies:**
- Hardcoded OpenAI SDK (`from openai import OpenAI`)
- Direct coupling to OpenAI API key

#### 2. **ChapterAnalysisService** (`chapter_analysis.py`)

**Purpose:** AI-powered entity extraction and chapter summarization

**Key Features:**
- Extract characters, places, terms from chapter content
- Generate chapter summaries
- JSON response parsing with fallback
- Entity name cleaning (removes decorative punctuation)

**Dependencies:**
- Inherits from `BaseAIService`
- Direct OpenAI API calls via `self.client.chat.completions.create()`
- Django models: `Language`

**Settings:**
- `ANALYSIS_MODEL` = "gpt-4o-mini"
- `ANALYSIS_MAX_TOKENS` = 2000
- `ANALYSIS_TEMPERATURE` = 0.1

#### 3. **ChapterTranslationService** (`chapter_translation.py`)

**Purpose:** AI-powered chapter translation with entity consistency

**Key Features:**
- Translate chapters with context awareness
- Entity consistency across translations
- Previous chapter context integration
- Entity mapping storage
- Retry logic and rate limiting
- Transaction safety

**Dependencies:**
- Inherits from `BaseAIService`
- Direct OpenAI API calls
- Django models: `Chapter`, `Language`, `TranslationJob`, `ChapterContext`, `BookEntity`

**Settings:**
- `TRANSLATION_MODEL` = "gpt-4o-mini"
- `TRANSLATION_MAX_TOKENS` = 16000
- `TRANSLATION_TEMPERATURE` = 0.3

### Current Usage Patterns

**Celery Tasks:**
- `books/tasks/chapter_translation.py` - Translation job processing
- `books/tasks/chapter_analysis.py` - (assumed) Analysis job processing

**Management Commands:**
- `test_build_translation_prompt.py`
- `test_entity_extraction.py`
- `test_extraction.py`

**Models:**
- `TranslationJob` - Tracks translation jobs
- `AnalysisJob` - Tracks analysis jobs
- `ChapterContext` - Stores extracted entities and summaries
- `BookEntity` - Stores entity translation mappings

---

## Problems with Current Design

### 1. **Tight Coupling to OpenAI**

- `BaseAIService` directly imports and initializes `OpenAI` client
- All API calls use OpenAI-specific methods
- No abstraction for different AI providers

### 2. **No Provider Abstraction**

- Services assume OpenAI response format
- JSON parsing logic embedded in service classes
- No interface for swapping backends

### 3. **Mixed Responsibilities**

- Services handle both AI communication AND Django model operations
- Entity storage logic mixed with translation logic
- Hard to test in isolation

### 4. **Configuration Inflexibility**

- Settings tied to specific model names
- No per-provider configuration
- Difficult to have different providers for different tasks

### 5. **Limited Error Handling**

- OpenAI-specific error handling
- No unified error abstraction across providers

### 6. **No Fallback Mechanism**

- Can't automatically fall back to alternative provider
- No provider health checking

---

## Proposed Architecture

### New Package Structure

```
myapp/ai_services/                    # New standalone package
‚îú‚îÄ‚îÄ __init__.py                       # Package exports
‚îú‚îÄ‚îÄ core/                             # Core abstractions
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base.py                       # BaseAIProvider abstract class
‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py                 # Unified exception hierarchy
‚îÇ   ‚îú‚îÄ‚îÄ models.py                     # Data models (request/response)
‚îÇ   ‚îî‚îÄ‚îÄ registry.py                   # Provider registry
‚îú‚îÄ‚îÄ providers/                        # AI provider implementations
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base_provider.py              # Abstract provider interface
‚îÇ   ‚îú‚îÄ‚îÄ openai_provider.py            # OpenAI implementation
‚îÇ   ‚îî‚îÄ‚îÄ gemini_provider.py            # Gemini implementation
‚îú‚îÄ‚îÄ services/                         # High-level service layer
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base_service.py               # Base service with provider selection
‚îÇ   ‚îú‚îÄ‚îÄ analysis.py                   # Analysis service (provider-agnostic)
‚îÇ   ‚îî‚îÄ‚îÄ translation.py                # Translation service (provider-agnostic)
‚îú‚îÄ‚îÄ prompts/                          # Prompt templates
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base.py                       # Prompt builder base class
‚îÇ   ‚îú‚îÄ‚îÄ analysis.py                   # Analysis prompt templates
‚îÇ   ‚îî‚îÄ‚îÄ translation.py                # Translation prompt templates
‚îî‚îÄ‚îÄ config.py                         # Configuration management

myapp/books/utils/                    # Keep for backwards compatibility
‚îú‚îÄ‚îÄ base_ai_service.py                # DEPRECATED - wrapper to new package
‚îú‚îÄ‚îÄ chapter_analysis.py               # DEPRECATED - wrapper to new package
‚îî‚îÄ‚îÄ chapter_translation.py            # DEPRECATED - wrapper to new package
```

### Core Components

#### 1. **BaseAIProvider** (Abstract Interface)

```python
# ai_services/core/base.py

from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
from .models import ChatMessage, ChatCompletionResponse

class BaseAIProvider(ABC):
    """
    Abstract base class for AI providers.

    All providers must implement these methods to be compatible
    with the AI services layer.
    """

    @abstractmethod
    def __init__(self, api_key: str, model: str, **kwargs):
        """Initialize provider with credentials and configuration"""
        pass

    @abstractmethod
    def chat_completion(
        self,
        messages: List[ChatMessage],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        response_format: Optional[str] = None,  # "json", "text"
        **kwargs
    ) -> ChatCompletionResponse:
        """
        Generate chat completion.

        Args:
            messages: List of chat messages
            max_tokens: Maximum tokens in response
            temperature: Randomness (0.0-1.0)
            response_format: Expected format ("json" or "text")

        Returns:
            ChatCompletionResponse with unified structure

        Raises:
            APIError: On API communication errors
            RateLimitError: On rate limit exceeded
            ValidationError: On invalid input
        """
        pass

    @abstractmethod
    def validate_settings(self) -> bool:
        """Validate provider settings and connectivity"""
        pass

    @abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about current model"""
        pass
```

#### 2. **Data Models** (Unified Request/Response)

```python
# ai_services/core/models.py

from dataclasses import dataclass
from typing import List, Optional, Dict, Any

@dataclass
class ChatMessage:
    """Unified chat message format"""
    role: str  # "system", "user", "assistant"
    content: str

@dataclass
class ChatCompletionResponse:
    """Unified completion response"""
    content: str
    model: str
    provider: str
    finish_reason: str
    usage: Dict[str, int]  # {"prompt_tokens": X, "completion_tokens": Y}
    raw_response: Optional[Any] = None  # Provider-specific raw response
```

#### 3. **Exception Hierarchy**

```python
# ai_services/core/exceptions.py

class AIServiceError(Exception):
    """Base exception for all AI service errors"""
    pass

class ProviderError(AIServiceError):
    """Base exception for provider-specific errors"""
    pass

class APIError(ProviderError):
    """API communication error"""
    pass

class RateLimitError(ProviderError):
    """Rate limit exceeded"""
    pass

class ValidationError(AIServiceError):
    """Input validation error"""
    pass

class ConfigurationError(AIServiceError):
    """Configuration error"""
    pass

class ProviderNotFoundError(AIServiceError):
    """Requested provider not found"""
    pass
```

#### 4. **Provider Registry**

```python
# ai_services/core/registry.py

from typing import Type, Dict, Optional
from .base import BaseAIProvider

class ProviderRegistry:
    """
    Central registry for AI providers.

    Allows dynamic provider registration and retrieval.
    """

    _providers: Dict[str, Type[BaseAIProvider]] = {}

    @classmethod
    def register(cls, name: str, provider_class: Type[BaseAIProvider]):
        """Register a provider"""
        cls._providers[name] = provider_class

    @classmethod
    def get(cls, name: str) -> Type[BaseAIProvider]:
        """Get a provider class by name"""
        if name not in cls._providers:
            raise ProviderNotFoundError(f"Provider '{name}' not found")
        return cls._providers[name]

    @classmethod
    def list_providers(cls) -> List[str]:
        """List all registered providers"""
        return list(cls._providers.keys())
```

#### 5. **OpenAI Provider Implementation**

```python
# ai_services/providers/openai_provider.py

from openai import OpenAI
from ai_services.core.base import BaseAIProvider
from ai_services.core.models import ChatMessage, ChatCompletionResponse
from ai_services.core.exceptions import APIError, RateLimitError

class OpenAIProvider(BaseAIProvider):
    """OpenAI API provider implementation"""

    def __init__(self, api_key: str, model: str = "gpt-4o-mini", **kwargs):
        self.api_key = api_key
        self.model = model
        self.client = OpenAI(api_key=api_key)
        self.kwargs = kwargs

    def chat_completion(
        self,
        messages: List[ChatMessage],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        response_format: Optional[str] = None,
        **kwargs
    ) -> ChatCompletionResponse:
        """Generate chat completion using OpenAI API"""

        # Convert unified messages to OpenAI format
        openai_messages = [
            {"role": msg.role, "content": msg.content}
            for msg in messages
        ]

        # Build request parameters
        params = {
            "model": self.model,
            "messages": openai_messages,
        }

        if max_tokens is not None:
            params["max_tokens"] = max_tokens
        if temperature is not None:
            params["temperature"] = temperature
        if response_format == "json":
            params["response_format"] = {"type": "json_object"}

        # Merge additional kwargs
        params.update(kwargs)

        try:
            response = self.client.chat.completions.create(**params)

            # Convert to unified response format
            return ChatCompletionResponse(
                content=response.choices[0].message.content,
                model=response.model,
                provider="openai",
                finish_reason=response.choices[0].finish_reason,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                },
                raw_response=response,
            )

        except Exception as e:
            error_msg = str(e).lower()
            if "rate limit" in error_msg:
                raise RateLimitError(str(e))
            raise APIError(f"OpenAI API error: {e}")

    def validate_settings(self) -> bool:
        """Validate OpenAI settings"""
        # Could ping API to verify key
        return bool(self.api_key)

    def get_model_info(self) -> Dict[str, Any]:
        """Get model information"""
        return {
            "provider": "openai",
            "model": self.model,
            "api_version": "v1",
        }
```

#### 6. **Gemini Provider Implementation**

```python
# ai_services/providers/gemini_provider.py

import google.generativeai as genai
from ai_services.core.base import BaseAIProvider
from ai_services.core.models import ChatMessage, ChatCompletionResponse
from ai_services.core.exceptions import APIError, RateLimitError

class GeminiProvider(BaseAIProvider):
    """Google Gemini API provider implementation"""

    def __init__(self, api_key: str, model: str = "gemini-2.0-flash-exp", **kwargs):
        self.api_key = api_key
        self.model_name = model
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model)
        self.kwargs = kwargs

    def chat_completion(
        self,
        messages: List[ChatMessage],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        response_format: Optional[str] = None,
        **kwargs
    ) -> ChatCompletionResponse:
        """Generate completion using Gemini API"""

        # Gemini uses a different message format
        # System messages go in config, user/assistant in history
        generation_config = {}

        if max_tokens is not None:
            generation_config["max_output_tokens"] = max_tokens
        if temperature is not None:
            generation_config["temperature"] = temperature
        if response_format == "json":
            generation_config["response_mime_type"] = "application/json"

        # Separate system messages from chat history
        system_instructions = []
        chat_history = []

        for msg in messages:
            if msg.role == "system":
                system_instructions.append(msg.content)
            else:
                # Map "user" and "assistant" to Gemini roles
                gemini_role = "user" if msg.role == "user" else "model"
                chat_history.append({
                    "role": gemini_role,
                    "parts": [msg.content]
                })

        try:
            # Create model with system instruction
            if system_instructions:
                model = genai.GenerativeModel(
                    self.model_name,
                    system_instruction="\n".join(system_instructions)
                )
            else:
                model = self.model

            # Generate response
            if chat_history:
                # Use last message as prompt, rest as history
                *history, last_message = chat_history
                chat = model.start_chat(history=history if history else None)
                response = chat.send_message(
                    last_message["parts"][0],
                    generation_config=generation_config
                )
            else:
                response = model.generate_content(
                    messages[0].content,
                    generation_config=generation_config
                )

            # Convert to unified response
            return ChatCompletionResponse(
                content=response.text,
                model=self.model_name,
                provider="gemini",
                finish_reason=response.candidates[0].finish_reason.name,
                usage={
                    "prompt_tokens": response.usage_metadata.prompt_token_count,
                    "completion_tokens": response.usage_metadata.candidates_token_count,
                },
                raw_response=response,
            )

        except Exception as e:
            error_msg = str(e).lower()
            if "quota" in error_msg or "rate" in error_msg:
                raise RateLimitError(str(e))
            raise APIError(f"Gemini API error: {e}")

    def validate_settings(self) -> bool:
        """Validate Gemini settings"""
        return bool(self.api_key)

    def get_model_info(self) -> Dict[str, Any]:
        """Get model information"""
        return {
            "provider": "gemini",
            "model": self.model_name,
            "api_version": "v1",
        }
```

#### 7. **Base Service Layer**

```python
# ai_services/services/base_service.py

from typing import Optional, Type
from ai_services.core.base import BaseAIProvider
from ai_services.core.registry import ProviderRegistry
from ai_services.core.exceptions import ConfigurationError

class BaseAIService:
    """
    Base service class with provider abstraction.

    Subclasses specify service-specific logic and delegate
    AI operations to the configured provider.
    """

    # Subclasses should override these
    DEFAULT_MODEL = "gpt-4o-mini"
    DEFAULT_MAX_TOKENS = 4000
    DEFAULT_TEMPERATURE = 0.3
    DEFAULT_PROVIDER = "openai"

    def __init__(
        self,
        provider_name: Optional[str] = None,
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        api_key: Optional[str] = None,
        **kwargs
    ):
        """
        Initialize service with provider.

        Args:
            provider_name: Provider to use ("openai", "gemini")
            model: Model name (provider-specific)
            max_tokens: Maximum tokens in response
            temperature: Randomness (0.0-1.0)
            api_key: API key for provider
            **kwargs: Additional provider-specific options
        """

        # Determine provider
        self.provider_name = provider_name or self._get_default_provider()

        # Get provider class from registry
        provider_class = ProviderRegistry.get(self.provider_name)

        # Get API key
        if api_key is None:
            api_key = self._get_api_key_for_provider(self.provider_name)

        if not api_key:
            raise ConfigurationError(
                f"No API key configured for provider '{self.provider_name}'"
            )

        # Set configuration
        self.model = model or self._get_default_model()
        self.max_tokens = max_tokens or self.DEFAULT_MAX_TOKENS
        self.temperature = temperature or self.DEFAULT_TEMPERATURE

        # Initialize provider
        self.provider = provider_class(
            api_key=api_key,
            model=self.model,
            **kwargs
        )

    def _get_default_provider(self) -> str:
        """Get default provider from settings or class default"""
        from django.conf import settings
        return getattr(settings, 'AI_DEFAULT_PROVIDER', self.DEFAULT_PROVIDER)

    def _get_default_model(self) -> str:
        """Get default model from settings or class default"""
        from django.conf import settings
        setting_name = f'{self.provider_name.upper()}_DEFAULT_MODEL'
        return getattr(settings, setting_name, self.DEFAULT_MODEL)

    def _get_api_key_for_provider(self, provider_name: str) -> Optional[str]:
        """Get API key for provider from settings"""
        from django.conf import settings

        key_mapping = {
            'openai': 'OPENAI_API_KEY',
            'gemini': 'GEMINI_API_KEY',
        }

        setting_name = key_mapping.get(provider_name)
        if not setting_name:
            raise ConfigurationError(f"Unknown provider: {provider_name}")

        return getattr(settings, setting_name, None)
```

#### 8. **Analysis Service (Provider-Agnostic)**

```python
# ai_services/services/analysis.py

import json
import logging
from typing import Dict, List
from .base_service import BaseAIService
from ai_services.core.models import ChatMessage
from ai_services.core.exceptions import APIError, ValidationError
from ai_services.prompts.analysis import AnalysisPromptBuilder

logger = logging.getLogger(__name__)

class AnalysisService(BaseAIService):
    """
    Provider-agnostic analysis service for entity extraction and summarization.
    """

    DEFAULT_MAX_TOKENS = 2000
    DEFAULT_TEMPERATURE = 0.1

    def extract_entities_and_summary(
        self,
        content: str,
        language_code: str = "zh"
    ) -> Dict[str, any]:
        """
        Extract entities and summary from content.

        Args:
            content: Text content to analyze
            language_code: Source language code

        Returns:
            Dict with keys: characters, places, terms, summary
        """

        # Build prompt using template
        prompt_builder = AnalysisPromptBuilder()
        prompt = prompt_builder.build(content, language_code)

        # Create message
        messages = [ChatMessage(role="user", content=prompt)]

        try:
            # Call provider (agnostic to OpenAI/Gemini)
            response = self.provider.chat_completion(
                messages=messages,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                response_format="json"
            )

            # Parse JSON response
            result = self._parse_json_response(response.content)
            self._validate_result(result)
            result = self._clean_entity_names(result)

            logger.info(
                f"Extracted entities via {self.provider_name}: "
                f"{len(result.get('characters', []))} chars, "
                f"{len(result.get('places', []))} places, "
                f"{len(result.get('terms', []))} terms"
            )

            return result

        except Exception as e:
            logger.error(f"Entity extraction failed: {e}")
            return self._get_fallback_result(content)

    def _parse_json_response(self, response_text: str) -> Dict:
        """Parse JSON response with cleaning"""
        # Same cleaning logic as current implementation
        cleaned = self._clean_json_response(response_text)
        return json.loads(cleaned)

    def _clean_json_response(self, response_text: str) -> str:
        """Clean JSON response (remove markdown, etc.)"""
        # Same as current implementation
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]

        response_text = response_text.strip()

        start_idx = response_text.find("{")
        end_idx = response_text.rfind("}")

        if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
            response_text = response_text[start_idx : end_idx + 1]

        return response_text

    def _validate_result(self, result: Dict) -> None:
        """Validate extraction result structure"""
        required_keys = ["characters", "places", "terms", "summary"]

        for key in required_keys:
            if key not in result:
                raise ValidationError(f"Missing required key: {key}")

        for key in ["characters", "places", "terms"]:
            if not isinstance(result[key], list):
                raise ValidationError(f"{key} must be a list")

        if not isinstance(result["summary"], str):
            raise ValidationError("summary must be a string")

    def _clean_entity_names(self, result: Dict) -> Dict:
        """Clean entity names (same as current implementation)"""
        # Same implementation as current
        decorative_chars = {
            '„Ää': '', '„Äã': '',
            '„Äå': '', '„Äç': '',
            '„Äé': '', '„Äè': '',
            '"': '', '"': '',
            '"': '', "'": '',
        }

        for category in ["characters", "places", "terms"]:
            if category in result and isinstance(result[category], list):
                cleaned = []
                for entity in result[category]:
                    if isinstance(entity, str):
                        cleaned_entity = entity
                        for old, new in decorative_chars.items():
                            cleaned_entity = cleaned_entity.replace(old, new)
                        cleaned_entity = cleaned_entity.strip()
                        if cleaned_entity:
                            cleaned.append(cleaned_entity)
                result[category] = cleaned

        return result

    def _get_fallback_result(self, content: str) -> Dict:
        """Return fallback result when extraction fails"""
        return {
            "characters": [],
            "places": [],
            "terms": [],
            "summary": content[:200] + "..." if len(content) > 200 else content,
        }
```

#### 9. **Translation Service (Provider-Agnostic)**

```python
# ai_services/services/translation.py

import json
import logging
import time
from typing import Dict, Tuple
from django.db import transaction

from .base_service import BaseAIService
from ai_services.core.models import ChatMessage
from ai_services.core.exceptions import APIError, ValidationError, RateLimitError
from ai_services.prompts.translation import TranslationPromptBuilder

logger = logging.getLogger("translation")

class TranslationService(BaseAIService):
    """
    Provider-agnostic translation service with entity consistency.
    """

    DEFAULT_MAX_TOKENS = 16000
    DEFAULT_TEMPERATURE = 0.3

    MAX_CONTENT_LENGTH = 8000
    MIN_CONTENT_LENGTH = 10
    MAX_RETRIES = 3
    RETRY_DELAY = 1

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._last_request_time = 0
        self._min_request_interval = 1

    def translate_chapter(
        self,
        source_chapter,  # Chapter model
        target_language_code: str
    ):
        """
        Translate a chapter to target language.

        This is the Django-specific wrapper that handles model operations.
        The actual translation is provider-agnostic.
        """
        from books.models import Language, Chapter

        # Validate input
        self._validate_chapter_content(source_chapter)
        target_language = Language.objects.get(code=target_language_code)

        # Rate limiting
        self._enforce_rate_limit()

        # Get context data
        context_data = self._gather_context(source_chapter, target_language)

        # Translate (provider-agnostic)
        translation_result = self._translate_with_context(
            title=source_chapter.title,
            content=source_chapter.content,
            source_language=source_chapter.book.language.name,
            target_language=target_language.name,
            context=context_data
        )

        # Create Django model
        translated_chapter = self._create_translated_chapter(
            source_chapter,
            target_language,
            translation_result
        )

        logger.info(
            f"Successfully translated chapter {source_chapter.id} to "
            f"{target_language_code} using {self.provider_name}"
        )

        return translated_chapter

    def _translate_with_context(
        self,
        title: str,
        content: str,
        source_language: str,
        target_language: str,
        context: Dict
    ) -> Dict:
        """
        Provider-agnostic translation with context.

        Returns:
            Dict with keys: title, content, entity_mappings, translator_notes
        """

        # Build prompt
        prompt_builder = TranslationPromptBuilder()
        prompt = prompt_builder.build(
            title=title,
            content=content,
            source_language=source_language,
            target_language=target_language,
            entities=context.get('entities', {}),
            previous_chapters=context.get('previous_chapters', [])
        )

        # Create message
        messages = [ChatMessage(role="user", content=prompt)]

        # Call provider with retry
        response_text = self._call_with_retry(messages)

        # Parse result
        return self._parse_translation_result(response_text)

    def _call_with_retry(self, messages: List[ChatMessage]) -> str:
        """Call provider with retry logic (provider-agnostic)"""
        last_exception = None

        for attempt in range(self.MAX_RETRIES):
            try:
                response = self.provider.chat_completion(
                    messages=messages,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature,
                    response_format="json"
                )

                if not response.content:
                    raise APIError("Empty response from provider")

                return response.content

            except RateLimitError as e:
                if attempt < self.MAX_RETRIES - 1:
                    sleep_time = self.RETRY_DELAY * (2 ** attempt)
                    logger.warning(f"Rate limit hit, retrying in {sleep_time}s")
                    time.sleep(sleep_time)
                    continue
                raise

            except Exception as e:
                last_exception = e
                if attempt < self.MAX_RETRIES - 1:
                    logger.warning(f"API call failed (attempt {attempt + 1}), retrying: {e}")
                    time.sleep(self.RETRY_DELAY)
                    continue

        raise APIError(f"Failed after {self.MAX_RETRIES} attempts: {last_exception}")

    def _gather_context(self, source_chapter, target_language) -> Dict:
        """Gather context data from Django models"""
        # Same implementation as current _get_previous_chapters_context
        # and _get_relevant_entities
        pass

    def _parse_translation_result(self, result_text: str) -> Dict:
        """Parse JSON translation result"""
        # Same implementation as current
        pass

    def _validate_chapter_content(self, chapter) -> None:
        """Validate chapter content"""
        # Same as current implementation
        pass

    def _enforce_rate_limit(self) -> None:
        """Simple rate limiting"""
        # Same as current implementation
        pass

    @transaction.atomic
    def _create_translated_chapter(
        self,
        source_chapter,
        target_language,
        translation_result: Dict
    ):
        """Create translated chapter in database"""
        # Same as current implementation
        pass
```

#### 10. **Prompt Templates**

```python
# ai_services/prompts/translation.py

class TranslationPromptBuilder:
    """
    Build translation prompts from templates.

    Separates prompt logic from service logic.
    """

    def build(
        self,
        title: str,
        content: str,
        source_language: str,
        target_language: str,
        entities: Dict = None,
        previous_chapters: List = None
    ) -> str:
        """Build translation prompt from template"""

        # Same prompt building logic as current _build_translation_prompt
        # but separated into reusable template

        prompt_parts = [
            f"# TRANSLATION TASK",
            f"Translate this chapter from **{source_language}** to **{target_language}**.",
            # ... rest of template
        ]

        return "\n".join(prompt_parts)
```

---

## Implementation Plan

### Phase 1: Core Infrastructure (Week 1) ‚úÖ COMPLETE

**Goal:** Build the foundation without breaking existing code

**Status:** ‚úÖ **100% Complete - Implemented 2025-12-06**

#### Tasks:

1. **Create new package structure** ‚úÖ
   - [x] Create `myapp/ai_services/` directory
   - [x] Set up `core/`, `providers/`, `services/`, `prompts/` subdirectories
   - [x] Create `__init__.py` files with proper exports

2. **Implement core abstractions** ‚úÖ
   - [x] `core/base.py` - BaseAIProvider abstract class
   - [x] `core/models.py` - ChatMessage, ChatCompletionResponse dataclasses
   - [x] `core/exceptions.py` - Exception hierarchy
   - [x] `core/registry.py` - ProviderRegistry

3. **Implement configuration** ‚úÖ
   - [x] `config.py` - Configuration management
   - [x] Add new settings to `settings.py`:
     ```python
     # AI Provider Configuration
     AI_DEFAULT_PROVIDER = 'openai'  # or 'gemini'

     # OpenAI Configuration
     OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')
     OPENAI_DEFAULT_MODEL = 'gpt-4o-mini'
     OPENAI_ANALYSIS_MODEL = 'gpt-4o-mini'
     OPENAI_ANALYSIS_MAX_TOKENS = 2000
     OPENAI_ANALYSIS_TEMPERATURE = 0.1
     OPENAI_TRANSLATION_MODEL = 'gpt-4o-mini'
     OPENAI_TRANSLATION_MAX_TOKENS = 16000
     OPENAI_TRANSLATION_TEMPERATURE = 0.3

     # Gemini Configuration
     GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', '')
     GEMINI_DEFAULT_MODEL = 'gemini-2.0-flash-exp'
     GEMINI_ANALYSIS_MODEL = 'gemini-2.0-flash-exp'
     GEMINI_ANALYSIS_MAX_TOKENS = 2000
     GEMINI_ANALYSIS_TEMPERATURE = 0.1
     GEMINI_TRANSLATION_MODEL = 'gemini-2.0-flash-exp'
     GEMINI_TRANSLATION_MAX_TOKENS = 16000
     GEMINI_TRANSLATION_TEMPERATURE = 0.3
     ```

4. **Write tests** ‚è≥
   - [ ] Test suite for `ProviderRegistry`
   - [ ] Test suite for exception hierarchy
   - [ ] Test suite for data models

**Deliverables:** ‚úÖ
- ‚úÖ Core package structure exists
- ‚è≥ Tests pass (test infrastructure created, needs dependency install)
- ‚úÖ No breaking changes to existing code

---

### Phase 2: OpenAI Provider (Week 2) ‚úÖ COMPLETE

**Goal:** Implement OpenAI provider and verify it works with existing functionality

**Status:** ‚úÖ **100% Complete - Implemented 2025-12-06**

#### Tasks:

1. **Implement OpenAI provider** ‚úÖ
   - [x] `providers/openai_provider.py` - Full OpenAI implementation
   - [x] Handle all OpenAI-specific API details
   - [x] Map OpenAI responses to unified format

2. **Register provider** ‚úÖ
   - [x] Auto-register OpenAI provider in `providers/__init__.py`
   - [x] Verify provider can be retrieved from registry

3. **Write tests** ‚è≥
   - [ ] Unit tests for OpenAI provider (mocked API)
   - [ ] Integration tests with real OpenAI API (skipped in CI)
   - [ ] Test error handling (rate limits, API errors)

4. **Validate against current behavior** ‚è≥
   - [ ] Create test comparing old vs new implementation
   - [ ] Verify identical outputs for same inputs

**Deliverables:** ‚úÖ
- ‚úÖ OpenAI provider fully functional
- ‚è≥ Tests pass (needs dependency install)
- ‚è≥ Behavior matches current implementation (ready for validation)

---

### Phase 3: Gemini Provider (Week 3) ‚úÖ COMPLETE

**Goal:** Implement Gemini provider as alternative backend

**Status:** ‚úÖ **100% Complete - Implemented 2025-12-06**

#### Tasks:

1. **Install Gemini SDK** ‚úÖ
   - [x] Add `google-generativeai` to `requirements/base.txt`
   - [x] Update documentation for Gemini setup

2. **Implement Gemini provider** ‚úÖ
   - [x] `providers/gemini_provider.py` - Full Gemini implementation
   - [x] Handle Gemini-specific message format
   - [x] Map Gemini responses to unified format
   - [x] Handle Gemini-specific errors

3. **Register provider** ‚úÖ
   - [x] Auto-register Gemini provider in `providers/__init__.py`

4. **Write tests** ‚è≥
   - [ ] Unit tests for Gemini provider (mocked API)
   - [ ] Integration tests with real Gemini API (skipped in CI)
   - [ ] Test error handling

5. **Cross-provider tests** ‚è≥
   - [ ] Test that can switch providers without code changes
   - [ ] Verify both providers produce valid outputs

**Deliverables:** ‚úÖ
- ‚úÖ Gemini provider fully functional
- ‚úÖ Can switch providers via configuration
- ‚è≥ Tests pass for both providers (ready to test)

---

### Phase 4: Service Layer (Week 4) üöß PARTIAL

**Goal:** Build provider-agnostic service layer

**Status:** üöß **75% Complete - Implemented 2025-12-06**

#### Tasks:

1. **Implement base service** ‚úÖ
   - [x] `services/base_service.py` - BaseAIService with provider selection
   - [x] Provider initialization logic
   - [x] Configuration loading

2. **Implement prompt builders** üöß
   - [x] `prompts/base.py` - BasePromptBuilder
   - [x] `prompts/analysis.py` - AnalysisPromptBuilder
   - [ ] `prompts/translation.py` - TranslationPromptBuilder (TODO)

3. **Implement analysis service** ‚úÖ
   - [x] `services/analysis.py` - AnalysisService (provider-agnostic)
   - [x] Port logic from current `ChapterAnalysisService`
   - [x] Remove provider-specific code

4. **Implement translation service** ‚è≥
   - [ ] `services/translation.py` - TranslationService (provider-agnostic) (TODO)
   - [ ] Port logic from current `ChapterTranslationService` (TODO)
   - [ ] Keep Django model operations separate (TODO)

5. **Write tests** ‚è≥
   - [ ] Test services with both OpenAI and Gemini
   - [ ] Test provider switching
   - [ ] Test fallback behavior

**Deliverables:** üöß
- ‚úÖ Provider-agnostic base service working
- ‚úÖ Analysis service complete and functional
- ‚è≥ Translation service (TODO - next session)
- ‚úÖ Prompt logic separated from service logic (for analysis)

---

### Phase 5: Integration & Migration (Week 5) üöß PARTIAL

**Goal:** Integrate new package with existing Django code

**Status:** üöß **30% Complete - Implemented 2025-12-06**

#### Tasks:

1. **Create compatibility layer** üöß
   - [ ] Update `books/utils/base_ai_service.py` to wrap new package (TODO)
   - [x] Create `books/utils/chapter_analysis_new.py` compatibility wrapper
   - [ ] Update `books/utils/chapter_translation.py` to wrap new package (TODO)
   - [x] Add deprecation warnings

2. **Update tasks** ‚è≥
   - [ ] Update `books/tasks/chapter_translation.py` to use new services (TODO)
   - [ ] Update `books/tasks/chapter_analysis.py` to use new services (TODO)
   - [ ] Verify Celery tasks work (TODO)

3. **Update management commands** ‚è≥
   - [ ] Update commands to use new services (TODO)
   - [ ] Test all commands (TODO)

4. **Add provider selection to admin** ‚è≥
   - [ ] Add field to select provider per job (optional enhancement) (TODO)
   - [ ] Default to global setting if not specified (TODO)

5. **Write integration tests** ‚è≥
   - [ ] End-to-end tests for translation workflow (TODO)
   - [ ] End-to-end tests for analysis workflow (TODO)
   - [ ] Test with both providers (TODO)

**Deliverables:** üöß
- üöß New package partially integrated with Django
- ‚è≥ All existing functionality works (needs validation)
- ‚úÖ Backward compatibility maintained (via wrapper)

---

### Phase 6: Documentation & Cleanup (Week 6) ‚úÖ COMPLETE

**Goal:** Document new architecture and clean up old code

**Status:** ‚úÖ **90% Complete - Implemented 2025-12-06**

#### Tasks:

1. **Write documentation** ‚úÖ
   - [x] Create `doc/AI_SERVICES_ARCHITECTURE_DIAGRAM.md`
   - [x] Create `doc/TRANSLATION_REFACTORING_PLAN.md` (master plan)
   - [x] Create `doc/TRANSLATION_REFACTORING_SUMMARY.md` (quick reference)
   - [x] Create `doc/AI_SERVICES_IMPLEMENTATION_STATUS.md` (status tracker)
   - [x] Document provider interface (in code docstrings)
   - [x] Document how to add new providers (in plan)
   - [x] Update CLAUDE.md with new architecture
   - [x] Create user guide for switching providers (in summary)

2. **Create examples** ‚úÖ
   - [x] Example: Using services directly (in documentation)
   - [x] Example: Provider switching (in summary)
   - [x] Test script: `myapp/test_ai_services.py`

3. **Add monitoring** ‚è≥
   - [x] Log provider used for each request (built into services)
   - [ ] Track success/failure rates per provider (TODO)
   - [ ] Add metrics for response times (TODO)

4. **Deprecation notices** ‚úÖ
   - [x] Add deprecation warnings to compatibility wrappers
   - [x] Create migration guide (in documentation)
   - [ ] Set timeline for removing old code (suggested: 3 months)

5. **Performance testing** ‚è≥
   - [ ] Benchmark OpenAI vs Gemini (TODO)
   - [ ] Document performance characteristics (TODO)
   - [ ] Optimize if needed (TODO)

**Deliverables:** ‚úÖ
- ‚úÖ Complete documentation (4 comprehensive docs)
- ‚úÖ Migration guide
- ‚è≥ Performance benchmarks (TODO)
- ‚úÖ Deprecation plan (in progress)

---

## Migration Strategy

### Backward Compatibility Approach

**Keep existing imports working during transition:**

```python
# books/utils/chapter_translation.py (compatibility wrapper)

import warnings
from ai_services.services.translation import TranslationService as NewTranslationService

class ChapterTranslationService(NewTranslationService):
    """
    DEPRECATED: This class is deprecated in favor of ai_services.services.translation.TranslationService

    This wrapper maintains backward compatibility. Please migrate to the new package.
    """

    def __init__(self):
        warnings.warn(
            "ChapterTranslationService is deprecated. Use ai_services.services.translation.TranslationService instead.",
            DeprecationWarning,
            stacklevel=2
        )
        # Initialize with OpenAI provider to maintain current behavior
        super().__init__(provider_name='openai')

# Keep old exception names for backward compatibility
TranslationError = NewTranslationService.TranslationError
TranslationValidationError = NewTranslationService.ValidationError
APIError = NewTranslationService.APIError
RateLimitError = NewTranslationService.RateLimitError
```

### Migration Timeline

**Week 1-4:** Build new package (no breaking changes)
**Week 5:** Add compatibility wrappers
**Week 6:** Documentation and deprecation notices
**Week 7-8:** Migrate existing code gradually
**Week 9:** Remove compatibility wrappers (breaking change in major version)

### Testing Strategy During Migration

1. **Dual testing:** Run tests against both old and new implementation
2. **Comparison tests:** Verify identical outputs
3. **Integration tests:** Ensure Django integration works
4. **Manual QA:** Test translation and analysis workflows

---

## Testing Strategy

### Unit Tests

**Provider Tests:**
```python
# tests/test_providers.py

class TestOpenAIProvider:
    def test_chat_completion(self, mock_openai_client):
        """Test basic chat completion"""
        provider = OpenAIProvider(api_key="test", model="gpt-4o-mini")
        messages = [ChatMessage(role="user", content="Hello")]
        response = provider.chat_completion(messages)
        assert response.content
        assert response.provider == "openai"

    def test_json_mode(self, mock_openai_client):
        """Test JSON response format"""
        provider = OpenAIProvider(api_key="test")
        messages = [ChatMessage(role="user", content="Return JSON")]
        response = provider.chat_completion(messages, response_format="json")
        # Verify JSON format requested

    def test_rate_limit_handling(self, mock_openai_client):
        """Test rate limit error handling"""
        # Mock rate limit error
        # Verify RateLimitError raised

class TestGeminiProvider:
    # Similar tests for Gemini
    pass
```

**Service Tests:**
```python
# tests/test_services.py

class TestAnalysisService:
    def test_extract_entities_openai(self, mock_openai_provider):
        """Test entity extraction with OpenAI"""
        service = AnalysisService(provider_name='openai')
        result = service.extract_entities_and_summary(
            content="Test content",
            language_code="zh"
        )
        assert 'characters' in result
        assert 'places' in result
        assert 'terms' in result
        assert 'summary' in result

    def test_extract_entities_gemini(self, mock_gemini_provider):
        """Test entity extraction with Gemini"""
        service = AnalysisService(provider_name='gemini')
        result = service.extract_entities_and_summary(
            content="Test content",
            language_code="zh"
        )
        # Same assertions

    def test_provider_switching(self):
        """Test switching providers without code changes"""
        # Test both providers produce valid results
        pass

class TestTranslationService:
    # Similar tests for translation
    pass
```

### Integration Tests

```python
# tests/integration/test_translation_workflow.py

class TestTranslationWorkflow:
    def test_full_translation_workflow(self):
        """Test complete translation workflow with database"""
        # Create source chapter
        # Run translation
        # Verify translated chapter created
        # Verify entity mappings stored

    def test_with_openai(self):
        """Test workflow with OpenAI provider"""
        pass

    def test_with_gemini(self):
        """Test workflow with Gemini provider"""
        pass
```

### Performance Tests

```python
# tests/performance/test_provider_performance.py

class TestProviderPerformance:
    def test_openai_latency(self):
        """Measure OpenAI response times"""
        pass

    def test_gemini_latency(self):
        """Measure Gemini response times"""
        pass

    def test_cost_comparison(self):
        """Compare token costs between providers"""
        pass
```

---

## Configuration

### Settings Structure

```python
# myapp/settings.py

# ============================================================================
# AI Services Configuration
# ============================================================================

# Default provider for all AI services
# Options: 'openai', 'gemini'
AI_DEFAULT_PROVIDER = os.getenv('AI_DEFAULT_PROVIDER', 'openai')

# ----------------------------------------------------------------------------
# OpenAI Configuration
# ----------------------------------------------------------------------------

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')

# Default model for OpenAI (used if not overridden by service)
OPENAI_DEFAULT_MODEL = 'gpt-4o-mini'

# Analysis service configuration (OpenAI)
OPENAI_ANALYSIS_MODEL = os.getenv('OPENAI_ANALYSIS_MODEL', 'gpt-4o-mini')
OPENAI_ANALYSIS_MAX_TOKENS = int(os.getenv('OPENAI_ANALYSIS_MAX_TOKENS', 2000))
OPENAI_ANALYSIS_TEMPERATURE = float(os.getenv('OPENAI_ANALYSIS_TEMPERATURE', 0.1))

# Translation service configuration (OpenAI)
OPENAI_TRANSLATION_MODEL = os.getenv('OPENAI_TRANSLATION_MODEL', 'gpt-4o-mini')
OPENAI_TRANSLATION_MAX_TOKENS = int(os.getenv('OPENAI_TRANSLATION_MAX_TOKENS', 16000))
OPENAI_TRANSLATION_TEMPERATURE = float(os.getenv('OPENAI_TRANSLATION_TEMPERATURE', 0.3))

# ----------------------------------------------------------------------------
# Gemini Configuration
# ----------------------------------------------------------------------------

GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', '')

# Default model for Gemini (used if not overridden by service)
GEMINI_DEFAULT_MODEL = 'gemini-2.0-flash-exp'

# Analysis service configuration (Gemini)
GEMINI_ANALYSIS_MODEL = os.getenv('GEMINI_ANALYSIS_MODEL', 'gemini-2.0-flash-exp')
GEMINI_ANALYSIS_MAX_TOKENS = int(os.getenv('GEMINI_ANALYSIS_MAX_TOKENS', 2000))
GEMINI_ANALYSIS_TEMPERATURE = float(os.getenv('GEMINI_ANALYSIS_TEMPERATURE', 0.1))

# Translation service configuration (Gemini)
GEMINI_TRANSLATION_MODEL = os.getenv('GEMINI_TRANSLATION_MODEL', 'gemini-2.0-flash-exp')
GEMINI_TRANSLATION_MAX_TOKENS = int(os.getenv('GEMINI_TRANSLATION_MAX_TOKENS', 16000))
GEMINI_TRANSLATION_TEMPERATURE = float(os.getenv('GEMINI_TRANSLATION_TEMPERATURE', 0.3))

# ----------------------------------------------------------------------------
# Service-specific Overrides
# ----------------------------------------------------------------------------

# Override provider for specific services (optional)
ANALYSIS_PROVIDER = os.getenv('ANALYSIS_PROVIDER', AI_DEFAULT_PROVIDER)
TRANSLATION_PROVIDER = os.getenv('TRANSLATION_PROVIDER', AI_DEFAULT_PROVIDER)
```

### Environment Variables

```bash
# .env

# Global settings
AI_DEFAULT_PROVIDER=openai  # or gemini

# OpenAI
OPENAI_API_KEY=sk-...
OPENAI_TRANSLATION_MODEL=gpt-4o-mini
OPENAI_ANALYSIS_MODEL=gpt-4o-mini

# Gemini
GEMINI_API_KEY=AI...
GEMINI_TRANSLATION_MODEL=gemini-2.0-flash-exp
GEMINI_ANALYSIS_MODEL=gemini-2.0-flash-exp

# Service-specific overrides
ANALYSIS_PROVIDER=gemini      # Use Gemini for analysis
TRANSLATION_PROVIDER=openai   # Use OpenAI for translation
```

---

## Timeline & Phases

### Overview

| Phase | Duration | Focus | Deliverables |
|-------|----------|-------|--------------|
| Phase 1 | Week 1 | Core Infrastructure | Package structure, abstractions, tests |
| Phase 2 | Week 2 | OpenAI Provider | Full OpenAI implementation, tests |
| Phase 3 | Week 3 | Gemini Provider | Full Gemini implementation, tests |
| Phase 4 | Week 4 | Service Layer | Provider-agnostic services |
| Phase 5 | Week 5 | Integration | Django integration, backward compatibility |
| Phase 6 | Week 6 | Documentation | Docs, examples, migration guide |

**Total Duration:** 6 weeks

### Key Milestones

- **Week 1 End:** Core package structure exists, no breaking changes
- **Week 2 End:** OpenAI provider working, matches current behavior
- **Week 3 End:** Gemini provider working, can switch providers
- **Week 4 End:** Services are provider-agnostic
- **Week 5 End:** Full Django integration, all tests pass
- **Week 6 End:** Production-ready with documentation

---

## Benefits of New Architecture

### 1. **Provider Flexibility**
- Easy to switch between OpenAI and Gemini
- Can add new providers (Anthropic, Cohere, local models)
- Per-service provider selection

### 2. **Better Testing**
- Mock providers for unit tests
- Test business logic without API calls
- Cross-provider compatibility tests

### 3. **Cost Optimization**
- Use cheaper provider for analysis
- Use better provider for translation
- Easy to benchmark and compare

### 4. **Maintainability**
- Clear separation of concerns
- Provider logic isolated
- Easier to debug issues

### 5. **Reliability**
- Automatic fallback to alternative provider
- Better error handling
- Provider health checking

### 6. **Extensibility**
- Add custom providers
- Custom prompt templates
- Service composition

---

## Risk Mitigation

### Risk 1: Breaking Existing Functionality

**Mitigation:**
- Maintain backward compatibility wrappers
- Extensive integration tests
- Gradual migration approach
- Feature flags for rollback

### Risk 2: Provider API Differences

**Mitigation:**
- Unified response format
- Provider-specific adapters
- Comprehensive error handling
- Fallback mechanisms

### Risk 3: Performance Degradation

**Mitigation:**
- Performance benchmarks
- Profiling during migration
- Optimize hot paths
- Monitor in production

### Risk 4: Configuration Complexity

**Mitigation:**
- Sensible defaults
- Clear documentation
- Validation on startup
- Environment variable support

---

## Future Enhancements

### Short Term (3-6 months)

1. **Provider health checking**
   - Ping providers before use
   - Automatic failover on errors

2. **Request caching**
   - Cache identical requests
   - Reduce API costs

3. **Batch processing**
   - Process multiple chapters in one request
   - Optimize for throughput

### Medium Term (6-12 months)

1. **Additional providers**
   - Anthropic Claude
   - Cohere
   - Local models (Ollama)

2. **Advanced prompt management**
   - A/B testing prompts
   - Prompt versioning
   - Analytics on prompt performance

3. **Hybrid approaches**
   - Use multiple providers in parallel
   - Voting/consensus mechanisms
   - Quality scoring

### Long Term (12+ months)

1. **Custom model fine-tuning**
   - Fine-tune on translation data
   - Domain-specific models

2. **Quality assurance**
   - Automatic translation quality scoring
   - Human-in-the-loop workflows

3. **Multi-modal support**
   - Image analysis
   - Audio transcription
   - Video processing

---

## Success Criteria

### Phase Completion Criteria

**Phase 1:**
- [ ] All core classes implemented
- [ ] Unit tests pass (>90% coverage)
- [ ] No breaking changes to existing code

**Phase 2:**
- [ ] OpenAI provider passes all tests
- [ ] Behavior matches current implementation
- [ ] Integration tests pass

**Phase 3:**
- [ ] Gemini provider passes all tests
- [ ] Can switch providers via config
- [ ] Cross-provider tests pass

**Phase 4:**
- [ ] Services are provider-agnostic
- [ ] Work with both OpenAI and Gemini
- [ ] Prompt templates separated

**Phase 5:**
- [ ] Django integration complete
- [ ] All existing functionality works
- [ ] Backward compatibility maintained

**Phase 6:**
- [ ] Documentation complete
- [ ] Examples working
- [ ] Migration guide available

### Overall Success Criteria

- [ ] All existing tests pass
- [ ] New tests added (>85% coverage)
- [ ] Can switch providers without code changes
- [ ] Performance equal or better
- [ ] Documentation complete
- [ ] No breaking changes for users
- [ ] Production deployment successful

---

## Appendix

### A. Dependencies

**New dependencies:**
```txt
# requirements/base.txt

# OpenAI (already exists)
openai>=1.0.0

# Gemini (NEW)
google-generativeai>=0.3.0
```

### B. File Checklist

**New files to create:**
- [ ] `myapp/ai_services/__init__.py`
- [ ] `myapp/ai_services/core/__init__.py`
- [ ] `myapp/ai_services/core/base.py`
- [ ] `myapp/ai_services/core/models.py`
- [ ] `myapp/ai_services/core/exceptions.py`
- [ ] `myapp/ai_services/core/registry.py`
- [ ] `myapp/ai_services/providers/__init__.py`
- [ ] `myapp/ai_services/providers/base_provider.py`
- [ ] `myapp/ai_services/providers/openai_provider.py`
- [ ] `myapp/ai_services/providers/gemini_provider.py`
- [ ] `myapp/ai_services/services/__init__.py`
- [ ] `myapp/ai_services/services/base_service.py`
- [ ] `myapp/ai_services/services/analysis.py`
- [ ] `myapp/ai_services/services/translation.py`
- [ ] `myapp/ai_services/prompts/__init__.py`
- [ ] `myapp/ai_services/prompts/base.py`
- [ ] `myapp/ai_services/prompts/analysis.py`
- [ ] `myapp/ai_services/prompts/translation.py`
- [ ] `myapp/ai_services/config.py`
- [ ] `tests/ai_services/__init__.py`
- [ ] `tests/ai_services/test_providers.py`
- [ ] `tests/ai_services/test_services.py`
- [ ] `tests/ai_services/test_registry.py`
- [ ] `tests/ai_services/integration/test_workflows.py`
- [ ] `doc/AI_SERVICES_ARCHITECTURE.md`

**Files to modify:**
- [ ] `myapp/myapp/settings.py` - Add new configuration
- [ ] `myapp/books/utils/base_ai_service.py` - Add compatibility wrapper
- [ ] `myapp/books/utils/chapter_analysis.py` - Add compatibility wrapper
- [ ] `myapp/books/utils/chapter_translation.py` - Add compatibility wrapper
- [ ] `myapp/books/tasks/chapter_translation.py` - Update to use new services
- [ ] `myapp/books/tasks/chapter_analysis.py` - Update to use new services
- [ ] `requirements/base.txt` - Add google-generativeai
- [ ] `doc/CLAUDE.md` - Update architecture documentation

### C. Reference Links

**OpenAI API:**
- Documentation: https://platform.openai.com/docs/api-reference
- Python SDK: https://github.com/openai/openai-python

**Gemini API:**
- Documentation: https://ai.google.dev/docs
- Python SDK: https://github.com/google/generative-ai-python

**Design Patterns:**
- Strategy Pattern: https://refactoring.guru/design-patterns/strategy
- Registry Pattern: https://www.martinfowler.com/eaaCatalog/registry.html
- Adapter Pattern: https://refactoring.guru/design-patterns/adapter

---

## Conclusion

This refactoring plan provides a comprehensive roadmap for transforming the current OpenAI-coupled translation system into a flexible, provider-agnostic architecture that supports both OpenAI and Gemini (and future providers).

The phased approach ensures:
- **No breaking changes** during development
- **Backward compatibility** during migration
- **Thorough testing** at each stage
- **Clear documentation** for users and developers
- **Future extensibility** for new providers and features

The new architecture separates concerns cleanly:
- **Providers** handle API communication
- **Services** handle business logic
- **Prompts** handle template generation
- **Django integration** handled separately

This makes the codebase more maintainable, testable, and flexible for future growth.
